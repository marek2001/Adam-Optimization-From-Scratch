{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from fastai.vision.all import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to test different optimizers using different optimizer evaluation functions\n",
    "#Run each optimizer across a) the Rosenbrock function, b) Local Minima filled functions, c) Bowl-Shaped functions, d) Plate-Shaped functions\n",
    "# and e) Functions with Steep drops\n",
    "\n",
    "#After running each optimizer across these functions, take the average across all functions and plot to determine average performance\n",
    "#Generate  x data for each test - use same initialization distribution to ensure reproducibility and fairness while testing\n",
    "\n",
    "#Function to calculate the loss (Euclidean distance between current point and global minima)\n",
    "#The loss will be the DISTANCE from the local minima to the gradient - we're measuring how fast the optimizer can reach this minima\n",
    "import math\n",
    "def computeEuclideanLoss(current_point, global_minima):\n",
    "    #Compute the squared difference between the two vectors\n",
    "    squared_difference = torch.float_power(torch.subtract(current_point, global_minima), 2.0)\n",
    "    #Add all squared differences, convert the resulting scalar to a float, and then take the square root\n",
    "    return math.sqrt(torch.sum(squared_difference).item())\n",
    "\n",
    "#Initialize x and y vectors - 3D functions will be written in the form z(x, y)\n",
    "#Both x and y will be initialized with normal distributions from ranges -5 to 5\n",
    "XYZ = torch.FloatTensor(3, 1).uniform_(-5, 5)\n",
    "\n",
    "#Import Custom Adam Implementation\n",
    "from CustomAdam import CustomAdam\n",
    "\n",
    "#Fit the given 3D function\n",
    "def fitFunction(epochs, xyz, minima, optimizer_function):\n",
    "    losses = []\n",
    "    #Iterate over number of epochs\n",
    "    for epoch in epochs:\n",
    "        #Set all gradients to zero - this is to prevent gradient accumulation as they do not reset per epoch\n",
    "        Optimizer.zero_grad()\n",
    "        #Compute the loss\n",
    "        loss = computeEuclideanLoss(xyz, minima)\n",
    "        #Compute the gradients\n",
    "        loss.backward()\n",
    "        #Run the optimizer\n",
    "        optimizer_function.step()\n",
    "        #Add loss to loss array\n",
    "        losses.append(loss)\n",
    "    #Return losses array\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionaries of all functions to be fit (equations) and their minima\n",
    "OPTIMIZER_TEST_FUNCTIONS = {\"ROSENBROCK\" : \"(1 - {})**2 + 100*({}-({}**2))**2\".format()}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dddb19642e9b394136ddc94a498b5755938c06ddbfeeb7fe6324263f04d0baab"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ml_mastery_replicates')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
